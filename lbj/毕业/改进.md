1.关于连续型稠密向量的优化
2.关于大稀疏向量的分组内交叉
3.关于CIN层数的优化
4.不经过sum pooling直接连接输出



model = xDeepFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(128,128,128,64,64), cin_layer_size=(64,64), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.001, l2_reg_embedding=0.001, l2_reg_dnn=0.002, l2_reg_cin=0.01, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task='binary')

CIN层数实验
t1 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64)                         test LogLoss 0.4862 test AUC 0.7614 15s 13 epoch val_loss: 0.4853 - val_binary_crossentropy: 0.4851

t2 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64，64)                           test LogLoss 0.4901 test AUC 0.7615 15s 13 epoch val_loss: 0.4855 - val_binary_crossentropy: 0.4852

t3 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64，)                             test LogLoss 0.4877 test AUC 0.7615 13s    14 epoch val_loss: 0.4862 - val_binary_crossentropy: 0.4859

t4 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64,64,64,64,64,64,64,64)   test LogLoss 0.4862 test AUC 0.7615 19s  12 epoch test LogLoss 0.4862 test AUC 0.7615 19s

sum pooling，mean pooling, 直接输出实验

t5 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64)                         test LogLoss 0.4881 test AUC 0.7614 15s 13 epoch val_loss: 0.4866 - val_binary_crossentropy: 0.4864

t6 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64,64,64,64,64,64,64,64)    test LogLoss 0.4866 test AUC 0.7614 15s 13 epoch val_loss: 0.4866 - val_binary_crossentropy: 0.4864


deepFM:  test LogLoss 0.4836 test AUC 0.7595 12 epoch val_loss: 0.4822 - val_binary_crossentropy: 0.4819

## CTR模型

1
## 批归一化（Batch Normalization）

深度神经网络随着训练的进行，网络中的参数也随着梯度下降在不停更新，这样就会产生两个方面的影响。首先，当底层网络中参数发生微弱变化时，每一层中网络中的神经元会通过前向传播将这些变化传递下去，使得微弱变化随着网络层数的加深被进一步放大。最直观的表现就是，通过线性函数变化和激活函数激活后的输出值越来越大，导致深层的网络非常容易进入饱和区，越来越难以训练。另一方面，参数的变化会造成每一层的输入分布发生改变，这就要求后层网络需要不停地去适应这种分布变化导致整个网络的学习速率变慢，这种现象也被称之为Covariate Shift。
BN(Batch Normalization)由Google于2015年提出，这是一个深度神经网络训练的技巧，主要就是用来解决上述两个方面的问题。从字面意思来理解BN，就是对每一批训练数据在训练时进行归一化处理。但在批训练的深度神经网络模型中，在训练某一个batch的数据{x1,x2,...,xn}时，不光可以对模型中输入层的数据进行求均值以及求方差做归一化处理，还可以对网络中间的任意一层进行归一化处理，实际上在应用BN时是对神经网络的多层输入进行处理。BN的数据处理步骤主要分为以下4步：

公式地址：https://zhuanlan.zhihu.com/p/34879333

第一步求出当前训练批次当前层输入数据的均值。第二步计算出当前训练批次当前层输入数据数据的方差。第三步使用求得的均值和方差对该层输入数据做归一化处理，获得0-1分布。其中𝜀是为了避免除数为0时所使用的微小正数。第四步进行尺度变换和偏移：将𝑥𝑖乘以𝛾调整数值大小，再加上𝛽增加偏移后得到𝑦𝑖，这里的𝛾是尺度因子，𝛽是平移因子。对于第四步的设计，主要是由于归一化后的𝑥𝑖基本会被限制在正态分布下，使得数据本身的表达能力下降。为优化该问题，作者又引入两个新的可学习超参数𝛾,𝛽，可以通过神经网络自己学习习得。特别是当 [公式] 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。

BN对于之前提出的两方面问题的解决非常的直接，首先输出值膨胀的问题会在BN归一化时被压缩，然后就是Covariate Shift会因为BN会强制将输入转换为近似正太分布从而保证每层的输入数据分布相对统一。除此之外在在深层神经网络，引入BN的作用其他优势也非常明显：1.因为对输入做了归一化，使得各输入数据特征的维度相对一致，不容易使得梯度下降学习曲折前进。所以可以使用更大的学习率，训练过程更加稳定，极大提高了训练速度。2.可以将bias置为0，因为BN的标准化过程会移除直流分量，所以不再需要bias。3.对权重初始化不再敏感，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了BN后，对与同一个输出节点相连的权重进行放缩，其标准差𝜎也会放缩同样的倍数，相除抵消。4.对权重的尺度不再敏感，理由同上，尺度统一由𝛾参数控制，在训练中决定。5.深层网络可以使用sigmoid和tanh了，因为BN抑制了梯度消失。6.Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合。

## 因素分解机

## wide&deep CTR模型结构
## 实验设置

## 实验环境配置

## 数据预处理

## 正则化参数调试

## 其他模型对比

## 不同激活函数实验

## CIN层数实验

## 连续型向量处理实验

由于稀疏离散型特征被广泛应用于CTR推荐模型，所以现在主流研究的CTR模型的模型输入格式都是稀疏的one-hot编码格式。但在实际运用中有时会遇见连续型输入数据，比如在推荐模型中被推荐对象的一些能力数值信息或者商品价格信息等。对于这种特征输入，一般有两种处理思路。第一种思路就是不保留其连续型数据的特征，最简单的方法就是对出现的每一个数值都做lableEncode转换，然后再转换成one-hot编码格式。但是如果数值是精度非常高的连续性数据，这样的转换出来的one-hot编码格式数据数据稀疏性非常严重，使得模型的训练代价陡增且准确率难以保障。另一种就是对连续性数据执行分桶策略，将连续性数据的数值范围划分成一个个区间，然后对区间进行lableEncode转换，这样可以保证出现最后出现类别不会过多，但这样也会因此损失一些精度信息。第二种思路则比较直接，不修改连续型数据的数据特征，直接使用连续型数据作为模型输入。
下面根据连续型数据作为模型输入，使用本文研究的基于压缩交互网络的CTR模型进行四类模型设计。第一类是对连续型数据进行归一化处理后，放弃参与显性特征交叉部分网络，仅参与DNN部分的高阶隐性特征交叉训练。第二类选择对连续型数据进行BN处理，依然只参与DNN部分对高阶隐形特征交叉训练。第三类将连续型特征离散化后作为id特征, embedding后与其他稀疏特征的embedding一起参与整个网络的交叉训练。第四类模型通过为每一个field下的连续型数据维护一个embedding vector，使用该field数据与vector的连接计算结果作为其最终的embedding表示。最后将该embedding与其他sparse feature的embedding一起参与整个CTR模型网络的交叉,该模型的输入层部分结构如下图--所示。

实验结果如下图--。

## CIN网络输出控制实验

图：文字借鉴--https://www.heywhale.com/mw/notebook/5d1c88819f53a9002ce3ab64