1.关于连续型稠密向量的优化
2.关于大稀疏向量的分组内交叉
3.关于CIN层数的优化
4.不经过sum pooling直接连接输出



model = xDeepFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(128,128,128,64,64), cin_layer_size=(64,64), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.001, l2_reg_embedding=0.001, l2_reg_dnn=0.01, l2_reg_cin=0.001, seed=1024, dnn_dropout=0.5, dnn_activation='relu', dnn_use_bn=False, task='binary')

CIN层数实验
t1 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64)                         test LogLoss 0.4862 test AUC 0.7614 15s 13 epoch val_loss: 0.4853 - val_binary_crossentropy: 0.4851

t2 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64，64)                           test LogLoss 0.4901 test AUC 0.7615 15s 13 epoch val_loss: 0.4855 - val_binary_crossentropy: 0.4852

t3 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64，)                             test LogLoss 0.4877 test AUC 0.7615 13s    14 epoch val_loss: 0.4862 - val_binary_crossentropy: 0.4859

t4 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64,64,64,64,64,64,64,64)   test LogLoss 0.4862 test AUC 0.7615 19s  12 epoch test LogLoss 0.4862 test AUC 0.7615 19s

sum pooling，mean pooling, 直接输出实验

t5 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64)                         test LogLoss 0.4881 test AUC 0.7614 15s 13 epoch val_loss: 0.4866 - val_binary_crossentropy: 0.4864

t6 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64,64,64,64,64,64,64,64)    test LogLoss 0.4866 test AUC 0.7614 15s 13 epoch val_loss: 0.4866 - val_binary_crossentropy: 0.4864


deepFM:  test LogLoss 0.4836 test AUC 0.7595 12 epoch val_loss: 0.4822 - val_binary_crossentropy: 0.4819

## 激活函数

## 梯度下降优化器

## 批归一化（Batch Normalization）

BN是由Google于2015年提出，这是一个深度神经网络训练的技巧，它不仅可以加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。所以目前BN已经成为几乎所有卷积神经网络的标配技巧了。从字面意思Batch Normalization（简称BN）来理解，就是对每一批数据进行归一化，确实如此。对于训练中某一个batch的数据{x1,x2,...,xn}，其中这个数据是可以输入也可以是网络中间的某一层输出。在BN出现之前，我们的归一化操作一般都在数据输入层，对输入的数据进行求均值以及求方差做归一化，但是BN的出现打破了这一个现象。我们可以在网络中任意一层进行归一化处理，因为我们现在所用的优化方法大多都是min-batch梯度下降，所以我们的归一化操作就成为Batch Normalization。
BN步骤主要分为4步：

1. 求每一个训练批次数据的均值
2. 求每一个训练批次数据的方差
3. 使用求得的均值和方差对该批次的训练数据做归一化，获得0-1分布。其中𝜀是为了避免除数为0时所使用的微小正数。
4. 尺度变换和偏移：将𝑥𝑖乘以𝛾调整数值大小，再加上𝛽增加偏移后得到𝑦𝑖，这里的𝛾是尺度因子，𝛽是平移因子。这一步是BN的精髓，由于归一化后的𝑥𝑖基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，我们引入两个新的参数：𝛾,𝛽。 𝛾和𝛽是在训练时网络自己学习得到的。

BN在深层神经网络的作用非常明显：若神经网络训练时遇到收敛速度较慢，或者“梯度爆炸”等无法训练的情况发生时都可以尝试用BN来解决。同时，常规使用情况下同样可以加入BN来加速模型训练，甚至提升模型精度。
## 实验设置

## 实验环境配置

## 数据预处理

## 正则化参数调试

## 其他模型对比

## 不同激活函数实验

## CIN层数实验

## 连续型向量处理实验

由于稀疏离散型特征被广泛应用于CTR推荐模型，所以现在主流研究的CTR模型的模型输入格式都是稀疏的one-hot编码格式。但在实际运用中有时会遇见连续型输入数据，比如在推荐模型中被推荐对象的一些能力数值信息或者商品价格信息等。对于这种特征输入，一般有两种处理思路。第一种思路就是不保留其连续型数据的特征，最简单的方法就是对出现的每一个数值都做lableEncode转换，然后再转换成one-hot编码格式。但是如果数值是精度非常高的连续性数据，这样的转换出来的one-hot编码格式数据数据稀疏性非常严重，使得模型的训练代价陡增且准确率难以保障。另一种就是对连续性数据执行分桶策略，将连续性数据的数值范围划分成一个个区间，然后对区间进行lableEncode转换，这样可以保证出现最后出现类别不会过多，但这样也会因此损失一些精度信息。第二种思路则比较直接，不修改连续型数据的数据特征，直接使用连续型数据作为模型输入。
下面根据连续型数据作为模型输入，使用本文研究的基于压缩交互网络的CTR模型进行四类模型设计。第一类是对连续型数据进行归一化处理后，放弃参与显性特征交叉部分网络，仅参与DNN部分的高阶隐性特征交叉训练。第二类选择对连续型数据进行BN处理，依然只参与DNN部分对高阶隐形特征交叉训练。第三类将连续型特征离散化后作为id特征, embedding后与其他稀疏特征的embedding一起参与整个网络的交叉训练。第四类模型通过为每一个field下的连续型数据维护一个embedding vector，使用该field数据与vector的连接计算结果作为其最终的embedding表示。最后将该embedding与其他sparse feature的embedding一起参与整个CTR模型网络的交叉,该模型的输入层部分结构如下图--所示。

实验结果如下图--。

## CIN网络输出控制实验

图：文字借鉴--https://www.heywhale.com/mw/notebook/5d1c88819f53a9002ce3ab64