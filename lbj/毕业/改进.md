1.关于连续型稠密向量的优化
2.关于大稀疏向量的分组内交叉
3.关于CIN层数的优化
4.不经过sum pooling直接连接输出



model = xDeepFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(128,128,128,64,64), cin_layer_size=(64,64), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.001, l2_reg_embedding=0.001, l2_reg_dnn=0.002, l2_reg_cin=0.01, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task='binary')

CIN层数实验
t1 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64)                         test LogLoss 0.4862 test AUC 0.7614 15s 13 epoch val_loss: 0.4853 - val_binary_crossentropy: 0.4851

t2 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64，64)                           test LogLoss 0.4901 test AUC 0.7615 15s 13 epoch val_loss: 0.4855 - val_binary_crossentropy: 0.4852

t3 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64，)                             test LogLoss 0.4877 test AUC 0.7615 13s    14 epoch val_loss: 0.4862 - val_binary_crossentropy: 0.4859

t4 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64,64,64,64,64,64,64,64)   test LogLoss 0.4862 test AUC 0.7615 19s  12 epoch test LogLoss 0.4862 test AUC 0.7615 19s

sum pooling，mean pooling, 直接输出实验

t5 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64)                         test LogLoss 0.4881 test AUC 0.7614 15s 13 epoch val_loss: 0.4866 - val_binary_crossentropy: 0.4864

t6 : dnn_hidden_units=(128,128,128,64,64 ), cin_layer_size=(64,64,64,64,64,64,64,64,64,64)    test LogLoss 0.4866 test AUC 0.7614 15s 13 epoch val_loss: 0.4866 - val_binary_crossentropy: 0.4864


deepFM:  test LogLoss 0.4836 test AUC 0.7595 12 epoch val_loss: 0.4822 - val_binary_crossentropy: 0.4819

不同激活函数实验

relu: test AUC 0.7669 test LogLoss 0.4765     val_loss: 0.5253 - val_binary_crossentropy: 0.4769
sigmoid: test AUC 0.721  test LogLoss 0.5082      val_loss: 0.5219 - val_binary_crossentropy: 0.5099
tanh:test LogLoss 0.4789 test AUC 0.7643  val_loss: 1.0562 - val_binary_crossentropy: 0.4787
## 激活函数

https://blog.csdn.net/tyhj_sf/article/details/79932893

### sigmoid

Sigmoid 是常用的非线性的激活函数，它的数学形式如下：
 
Sigmoid的几何图像如下：

特点：它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
缺点：1.在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。首先来看Sigmoid函数的导数，如下图所示：

如果我们初始化神经网络的权值为 [ 0 , 1 ] [0,1][0,1] 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 ( 1 , + ∞ ) (1,+∞)(1,+∞) 区间内的值，则会出现梯度爆炸情况。
2.Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：如x &gt; 0 ,   f = w T x + b x0, \ f= w^Tx+bx>0, f=w 
T
 x+b,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。
缺点3：其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。

### tanh

tanh函数解析式：
 

tanh函数及其导数的几何图像如下图：

tanh读作Hyperbolic Tangent，它解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。

### Relu

Relu函数
Relu函数的解析式：
R e l u = m a x ( 0 , x ) Relu=max(0,x)
Relu=max(0,x)

Relu函数及其导数的图像如下图所示：

ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：
1）解决了梯度消失问题 (在正区间)
2）计算速度非常快，只需要判断输入是否大于0
3）收敛速度远快于sigmoid和tanh

ReLU也有几个需要特别注意的问题：
1）ReLU的输出不是zero-centered
2）Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

## 深度神经网络

## 批归一化（Batch Normalization）

深度神经网络随着训练的进行，网络中的参数也随着梯度下降在不停更新，这样就会产生两个方面的影响。首先，当底层网络中参数发生微弱变化时，每一层中网络中的神经元会通过前向传播将这些变化传递下去，使得微弱变化随着网络层数的加深被进一步放大。最直观的表现就是，通过线性函数变化和激活函数激活后的输出值越来越大，导致深层的网络非常容易进入饱和区，越来越难以训练。另一方面，参数的变化会造成每一层的输入分布发生改变，这就要求后层网络需要不停地去适应这种分布变化导致整个网络的学习速率变慢，这种现象也被称之为Covariate Shift。
BN(Batch Normalization)由Google于2015年提出，这是一个深度神经网络训练的技巧，主要就是用来解决上述两个方面的问题。从字面意思来理解BN，就是对每一批训练数据在训练时进行归一化处理。但在批训练的深度神经网络模型中，在训练某一个batch的数据{x1,x2,...,xn}时，不光可以对模型中输入层的数据进行求均值以及求方差做归一化处理，还可以对网络中间的任意一层进行归一化处理，实际上在应用BN时是对神经网络的多层输入进行处理。BN的数据处理步骤主要分为以下4步：

公式地址：https://zhuanlan.zhihu.com/p/34879333

第一步求出当前训练批次当前层输入数据的均值。第二步计算出当前训练批次当前层输入数据数据的方差。第三步使用求得的均值和方差对该层输入数据做归一化处理，获得0-1分布。其中𝜀是为了避免除数为0时所使用的微小正数。第四步进行尺度变换和偏移：将𝑥𝑖乘以𝛾调整数值大小，再加上𝛽增加偏移后得到𝑦𝑖，这里的𝛾是尺度因子，𝛽是平移因子。对于第四步的设计，主要是由于归一化后的𝑥𝑖基本会被限制在正态分布下，使得数据本身的表达能力下降。为优化该问题，作者又引入两个新的可学习超参数𝛾,𝛽，可以通过神经网络自己学习习得。特别是当 [公式] 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。

BN对于之前提出的两方面问题的解决非常的直接，首先输出值膨胀的问题会在BN归一化时被压缩，然后就是Covariate Shift会因为BN会强制将输入转换为近似正太分布从而保证每层的输入数据分布相对统一。除此之外在在深层神经网络，引入BN的作用其他优势也非常明显：1.因为对输入做了归一化，使得各输入数据特征的维度相对一致，不容易使得梯度下降学习曲折前进。所以可以使用更大的学习率，训练过程更加稳定，极大提高了训练速度。2.可以将bias置为0，因为BN的标准化过程会移除直流分量，所以不再需要bias。3.对权重初始化不再敏感，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了BN后，对与同一个输出节点相连的权重进行放缩，其标准差𝜎也会放缩同样的倍数，相除抵消。4.对权重的尺度不再敏感，理由同上，尺度统一由𝛾参数控制，在训练中决定。5.深层网络可以使用sigmoid和tanh了，因为BN抑制了梯度消失。6.Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合。

## CTR模型
## 因素分解机

## wide&deep CTR模型结构

## 实验设置

## 实验环境配置

## 数据预处理

## 正则化参数调试

## 其他模型对比

## 不同激活函数实验

## CIN层数实验

## 连续型向量处理实验

由于稀疏离散型特征被广泛应用于CTR推荐模型，所以现在主流研究的CTR模型的模型输入格式都是稀疏的one-hot编码格式。但在实际运用中有时会遇见连续型输入数据，比如在推荐模型中被推荐对象的一些能力数值信息或者商品价格信息等。对于这种特征输入，一般有两种处理思路。第一种思路就是不保留其连续型数据的特征，最简单的方法就是对出现的每一个数值都做lableEncode转换，然后再转换成one-hot编码格式。但是如果数值是精度非常高的连续性数据，这样的转换出来的one-hot编码格式数据数据稀疏性非常严重，使得模型的训练代价陡增且准确率难以保障。另一种就是对连续性数据执行分桶策略，将连续性数据的数值范围划分成一个个区间，然后对区间进行lableEncode转换，这样可以保证出现最后出现类别不会过多，但这样也会因此损失一些精度信息。第二种思路则比较直接，不修改连续型数据的数据特征，直接使用连续型数据作为模型输入。
下面根据连续型数据作为模型输入，使用本文研究的基于压缩交互网络的CTR模型进行四类模型设计。第一类是对连续型数据进行归一化处理后，放弃参与显性特征交叉部分网络，仅参与DNN部分的高阶隐性特征交叉训练。第二类选择对连续型数据进行BN处理，依然只参与DNN部分对高阶隐形特征交叉训练。第三类将连续型特征离散化后作为id特征, embedding后与其他稀疏特征的embedding一起参与整个网络的交叉训练。第四类模型通过为每一个field下的连续型数据维护一个embedding vector，使用该field数据与vector的连接计算结果作为其最终的embedding表示。最后将该embedding与其他sparse feature的embedding一起参与整个CTR模型网络的交叉,该模型的输入层部分结构如下图--所示。

实验结果如下图--。

## CIN网络输出控制实验

图：文字借鉴--https://www.heywhale.com/mw/notebook/5d1c88819f53a9002ce3ab64