# 最佳实践

## 确定数据源来源数据最大延迟时间？

延迟时间来自于采集间隔，databus消费积压间隔，多阶段kafka聚合计算间隔。

1. 使用事件时间，采集时间延迟能得到解决。
2. databus的key按照server_id分区(不使用interface_num是担心会发生变动)，这样在flink按server_id分组计算时，顺序正确或许watermark会按消费顺序正常增长，解决消费积压延迟问题。(如何确保顺序正确)
3. 使用事件时间可以解决计算问题，但查询延迟不能保证。(如果是无状态的计算流，如简单的连接操作，不用担心计算问题)

## mysql维表的维护

也可以通过flink的动态表，主键映射唯一索引，维护维表的更新。这样维表也有watermark，通过这种方式可以排除处理时间的不确定性，以达到还原历史数据的目的。如果不在意此不确定性，直接采用最新快照即处理时间来联接无法物化转换为flink动态表的mysql,hbase等维表。

flink与纯ck方案区别：

1. 新方案采用append-only的插入ck数据，不用考虑2的相关的系列问题。
2. 如遇见动态更改修正数据，物化视图可控制，即可以比较麻烦的修改数据。(mysql,hive数据源应该是可控制的，ck对删除有延迟，无upsert，对此问题的解决方案是怎样的？)
3. 延迟时间，消息堆积等问题可解决

## bsql坑

维表join多字段判等，报错信息不明确。
插入时，需保证insert语句中的每个字段顺序和表顺序一致，并不会按照名字相等匹配，只会按照顺序匹配插入。
插入日志监控，有时候输入会变double
window聚合是左闭右开，上个窗口的end时间，到后一个窗口就会落后一个窗口
日志输出有采样，且影响监控
延迟不可控，分钟级延迟，3m居然才能保证大部分延迟对齐，5m保证完全延迟对齐(gd-mock计算的问题)
多个bsql任务接同一个kafka，不同任务延迟不同且高。都在同一个又不好维护